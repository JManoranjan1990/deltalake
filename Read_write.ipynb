{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating spark session by setting required configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001B47906AE00>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import*\n",
    "\n",
    "warehouse=\"D:\\\\warehouse\"\n",
    "builder= SparkSession.builder.appName(\"deltaexploration\")\\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "    .config(\"spak.sql.warehouse.dir\",warehouse)\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder)\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data by setting schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Schema=\"InvoiceNo String,StockCode String,Description String,Quantity Int,InvoiceDate String,UnitPrice Double,CustomerID long,Country String\"\n",
    "data=spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .schema(Schema)\\\n",
    "    .option(\"path\",\"C:\\\\Users\\\\Admin\\\\Downloads\\\\order_data.csv\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are going to see \n",
    "1.how we can save the data as delta table\n",
    "2.how we can create managed and unmanged delta table using hive metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write delta table to output directory, we are using append mode here.\n",
    "data.write.format('delta').mode('append').save(\"\\\\Lakehouse\\\\Delta\\\\output\\\\orders_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to create a unmanaged table using sql way \n",
    "# Let us create a database specific to this deltalake exploration\n",
    "\n",
    "spark.sql(\"create database deltadb\")\n",
    "spark.sql(\"use deltadb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are loading the table from  the external location we wrote earlier and it is an external table ,if we drop the table from metastore we still have data in external location\n",
    "\n",
    "spark.sql(\"\"\"CREATE TABLE deltadb.orders(\n",
    "          InvoiceNo STRING,\n",
    "          StockCode STRING,\n",
    "          Description STRING,\n",
    "          Quantity INT,\n",
    "          InvoiceDate STRING,\n",
    "          UnitPrice DOUBLE,\n",
    "          CustomerID LONG,\n",
    "          Country STRING) \n",
    "          USING DELTA \n",
    "          LOCATION '\\\\\\\\Lakehouse\\\\\\\\Delta\\\\\\\\output\\\\\\\\orders_delta' \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "| deltadb|   orders|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check if the table is created ?\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|   536378|     null|PACK OF 60 DINOSA...|      24|01-12-2010 9.37|     0.55|     14688|United Kingdom|\n",
      "|   536378|     null|PACK OF 60 PINK P...|      24|01-12-2010 9.37|     0.55|     14688|United Kingdom|\n",
      "|   536378|    84991|60 TEATIME FAIRY ...|      24|01-12-2010 9.37|     0.55|     14688|United Kingdom|\n",
      "|   536378|   84519A|TOMATO CHARLIE+LO...|       6|01-12-2010 9.37|     2.95|     14688|United Kingdom|\n",
      "|   536378|   85183B|CHARLIE & LOLA WA...|      48|01-12-2010 9.37|     1.25|     14688|United Kingdom|\n",
      "|   536378|   85071B|RED CHARLIE+LOLA ...|      96|01-12-2010 9.37|     0.38|     14688|United Kingdom|\n",
      "|   536378|    21931|JUMBO STORAGE BAG...|      10|01-12-2010 9.37|     1.95|     14688|United Kingdom|\n",
      "|   536378|    21929|JUMBO BAG PINK VI...|      10|01-12-2010 9.37|     1.95|     14688|United Kingdom|\n",
      "|   536380|    22961|JAM MAKING SET PR...|      24|01-12-2010 9.41|     1.45|     17809|United Kingdom|\n",
      "|   536381|    22139|RETROSPOT TEA SET...|      23|01-12-2010 9.41|     4.25|     15311|United Kingdom|\n",
      "|   536381|    84854| GIRLY PINK TOOL SET|       5|01-12-2010 9.41|     4.95|     15311|United Kingdom|\n",
      "|   536381|    22411|JUMBO SHOPPER VIN...|      10|01-12-2010 9.41|     1.95|     15311|United Kingdom|\n",
      "|   536381|    82567|AIRLINE LOUNGE,ME...|       2|01-12-2010 9.41|      2.1|     15311|United Kingdom|\n",
      "|   536381|    21672|WHITE SPOT RED CE...|       6|01-12-2010 9.41|     1.25|     15311|United Kingdom|\n",
      "|   536381|    22774|RED DRAWER KNOB A...|      24|01-12-2010 9.41|     1.25|     15311|United Kingdom|\n",
      "|   536381|    22771|CLEAR DRAWER KNOB...|      24|01-12-2010 9.41|     1.25|     15311|United Kingdom|\n",
      "|   536381|    71270|     PHOTO CLIP LINE|       1|01-12-2010 9.41|     1.25|     15311|United Kingdom|\n",
      "|   536381|    22262|FELT EGG COSY CHI...|       1|01-12-2010 9.41|     0.85|     15311|United Kingdom|\n",
      "|   536381|    22637|PIGGY BANK RETROS...|       1|01-12-2010 9.41|     2.55|     15311|United Kingdom|\n",
      "|   536381|    21934|  SKULL SHOULDER BAG|      10|01-12-2010 9.41|     1.65|     15311|United Kingdom|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let us access the data\n",
    "\n",
    "spark.sql(\"select * from deltadb.orders \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                        |comment|\n",
      "+----------------------------+-----------------------------------------------------------------+-------+\n",
      "|InvoiceNo                   |string                                                           |       |\n",
      "|StockCode                   |string                                                           |       |\n",
      "|Description                 |string                                                           |       |\n",
      "|Quantity                    |int                                                              |       |\n",
      "|InvoiceDate                 |string                                                           |       |\n",
      "|UnitPrice                   |double                                                           |       |\n",
      "|CustomerID                  |bigint                                                           |       |\n",
      "|Country                     |string                                                           |       |\n",
      "|                            |                                                                 |       |\n",
      "|# Partitioning              |                                                                 |       |\n",
      "|Not partitioned             |                                                                 |       |\n",
      "|                            |                                                                 |       |\n",
      "|# Detailed Table Information|                                                                 |       |\n",
      "|Name                        |deltadb.orders                                                   |       |\n",
      "|Location                    |file:/Lakehouse/Delta/output/orders_delta                        |       |\n",
      "|Provider                    |delta                                                            |       |\n",
      "|Table Properties            |[Type=EXTERNAL,delta.minReaderVersion=1,delta.minWriterVersion=2]|       |\n",
      "+----------------------------+-----------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to get information relating to table and we can see it as an external table\n",
    "spark.sql(f\"DESCRIBE  FORMATTED {'deltadb.orders'}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us see how we can use dataframe writer API which can simultaneously create table and instert from spark datafrmae \n",
    "# -This makes our life easy and metadata-datatypes will be inferred from spark dataframe and it will be a managed table and will be located in spark warehouse directory\n",
    "\n",
    "data.write.format('delta').mode('append').saveAsTable(\"deltadb.orders2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                       |comment|\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|InvoiceNo                   |string                                                          |       |\n",
      "|StockCode                   |string                                                          |       |\n",
      "|Description                 |string                                                          |       |\n",
      "|Quantity                    |int                                                             |       |\n",
      "|InvoiceDate                 |string                                                          |       |\n",
      "|UnitPrice                   |double                                                          |       |\n",
      "|CustomerID                  |bigint                                                          |       |\n",
      "|Country                     |string                                                          |       |\n",
      "|                            |                                                                |       |\n",
      "|# Partitioning              |                                                                |       |\n",
      "|Not partitioned             |                                                                |       |\n",
      "|                            |                                                                |       |\n",
      "|# Detailed Table Information|                                                                |       |\n",
      "|Name                        |deltadb.orders2                                                 |       |\n",
      "|Location                    |file:/d:/Lakehouse/spark-warehouse/deltadb.db/orders2           |       |\n",
      "|Provider                    |delta                                                           |       |\n",
      "|Table Properties            |[Type=MANAGED,delta.minReaderVersion=1,delta.minWriterVersion=2]|       |\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"select * from deltadb.orders1\")\n",
    "spark.sql(f\"DESCRIBE  FORMATTED {'deltadb.orders2'}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deltalake-nKKbuMMH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
