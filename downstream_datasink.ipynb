{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "717d6c49-3643-46ba-a1bd-9c22371f52da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Trigger type is default -which is contionusly running microbatches as one batch finishes\n",
    "This Notebok is reading data from delta as stream.when ever there is any change in delta table in the form of append this stream application captures that increamental data and writes it to sink\n",
    "Drawback of this use case is -it cannot handle any changes in the upstream source table that is not an append and throws an exception\n",
    "\n",
    "Let us see two  scenarios -\n",
    "1.when we start an applcation it reads the intial snapshot of data.In this case we had manually inserted 10 records from orders data -refer upstream_data writer notebook .in first microbatch 10 recods are processed from the inital data and in the following microbatch the new 100 records are  processed.\n",
    " \n",
    "streaming_query.recentProgress will help in getting the status of all the micro batches  - for example -details of first micro batch.\n",
    "\n",
    "We can check the number of rows processed using numInputRows key in this dictionary\n",
    "\n",
    "{'id': 'fb3a7f06-e606-416d-964b-7b257c03db92',\n",
    "  'runId': '238b82b4-2b3d-4468-8618-6984e01e50ae',\n",
    "  'name': None,\n",
    "  'timestamp': '2024-10-19T12:26:32.123Z',\n",
    "  'batchId': 0,\n",
    "  'numInputRows': 10,\n",
    "  'inputRowsPerSecond': 0.0,\n",
    "  'processedRowsPerSecond': 1.5276504735716467,\n",
    "  'durationMs': {'addBatch': 4234,\n",
    "   'commitOffsets': 118,\n",
    "   'getBatch': 54,\n",
    "   'latestOffset': 1749,\n",
    "   'queryPlanning': 43,\n",
    "   'triggerExecution': 6546,\n",
    "   'walCommit': 181},\n",
    "  'stateOperators': [],\n",
    "  'sources': [{'description': 'DeltaSource[dbfs:/FileStore/tables/delta/orders]',\n",
    "    'startOffset': None,\n",
    "    'endOffset': {'sourceVersion': 1,\n",
    "     'reservoirId': '4af1a0f9-b71a-4acc-b926-00fefbed9ce1',\n",
    "     'reservoirVersion': 0,\n",
    "     'index': 0,\n",
    "     'isStartingVersion': True},\n",
    "    'latestOffset': None,\n",
    "    'numInputRows': 10,\n",
    "    'inputRowsPerSecond': 0.0,\n",
    "    'processedRowsPerSecond': 1.5276504735716467,\n",
    "    'metrics': {'numBytesOutstanding': '0', 'numFilesOutstanding': '0'}}],\n",
    "  'sink': {'description': 'DeltaSink[/FileStore/tables/delta/stream_orders]',\n",
    "   'numOutputRows': -1}}\n",
    "\n",
    "\n",
    "\n",
    "similarly when the source table gets the new data ,straem query reads it as a microbatch and writes it to sink.\n",
    "{'id': 'fb3a7f06-e606-416d-964b-7b257c03db92',\n",
    "  'runId': '238b82b4-2b3d-4468-8618-6984e01e50ae',\n",
    "  'name': None,\n",
    "  'timestamp': '2024-10-19T12:32:24.931Z',\n",
    "  'batchId': 1,\n",
    "  'numInputRows': 100,\n",
    "  'inputRowsPerSecond': 181.15942028985506,\n",
    "  'processedRowsPerSecond': 16.570008285004143,\n",
    "  'durationMs': {'addBatch': 5283,\n",
    "   'commitOffsets': 121,\n",
    "   'getBatch': 192,\n",
    "   'latestOffset': 152,\n",
    "   'queryPlanning': 19,\n",
    "   'triggerExecution': 6035,\n",
    "   'walCommit': 261},\n",
    "  'stateOperators': [],\n",
    "  'sources': [{'description': 'DeltaSource[dbfs:/FileStore/tables/delta/orders]',\n",
    "    'startOffset': {'sourceVersion': 1,\n",
    "     'reservoirId': '4af1a0f9-b71a-4acc-b926-00fefbed9ce1',\n",
    "     'reservoirVersion': 0,\n",
    "     'index': 0,\n",
    "     'isStartingVersion': True},\n",
    "    'endOffset': {'sourceVersion': 1,\n",
    "     'reservoirId': '4af1a0f9-b71a-4acc-b926-00fefbed9ce1',\n",
    "     'reservoirVersion': 2,\n",
    "     'index': -1,\n",
    "     'isStartingVersion': False},\n",
    "    'latestOffset': None,\n",
    "    'numInputRows': 100,\n",
    "    'inputRowsPerSecond': 181.15942028985506,\n",
    "    'processedRowsPerSecond': 16.570008285004143,\n",
    "    'metrics': {'numBytesOutstanding': '0', 'numFilesOutstanding': '0'}}],\n",
    "  'sink': {'description': 'DeltaSink[/FileStore/tables/delta/stream_orders]',\n",
    "   'numOutputRows': -1}}\n",
    "\n",
    "\n",
    "Scenario 2 -when there is an update to the source table the below query fails and it is not propogated to down stream. we will get the below error\n",
    "\n",
    "com.databricks.sql.transaction.tahoe.DeltaUnsupportedOperationException: Detected a data update (for example part-00000-f4791fdb-58a9-495c-b2e9-a229c8c0eb05-c000.snappy.parquet) in the source table at version 3. This is currently not supported. If you'd like to ignore updates, set the option 'skipChangeCommits' to 'true'. If you would like the data update to be reflected, please restart this query with a fresh checkpoint directory. The source table can be found at path dbfs:/FileStore/tables/delta/orders.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5184405-4cee-487a-9f2e-79c4f5767327",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42daca9f-a166-44f7-a16b-eb57026378c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders_stream=spark.readStream\\\n",
    "    .format(\"delta\")\\\n",
    "    .option(\"path\",\"/FileStore/tables/delta/orders\")\\\n",
    "    .load()\n",
    "\n",
    "orders_stream1=orders_stream.withColumn(\"Processing_timestamp\",current_timestamp())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5ee94b7-0957-4505-afc5-f843d8441253",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path=\"/FileStore/tables/delta/stream_orders\"\n",
    "streaming_query=orders_stream1.writeStream\\\n",
    "    .format(\"delta\")\\\n",
    "    .option(\"path\",path)\\\n",
    "    .option(\"checkpointLocation\",f\"{path}/_checkpoint\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00bdb4e4-dc55-4d91-9ece-11dc61861cfb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "downstream_datasink",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
